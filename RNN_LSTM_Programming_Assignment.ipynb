{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Y326s/ECE6397HW/blob/main/RNN_LSTM_Programming_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assignment-header"
      },
      "source": [
        "# RNN and LSTM Programming Assignment\n",
        "\n",
        "## Learning Objectives\n",
        "By completing this assignment, you will:\n",
        "\n",
        "1. **Implement RNNs from scratch** and understand their basic mechanics\n",
        "2. **Observe vanishing/exploding gradient problems** through empirical experiments\n",
        "3. **Implement LSTMs** and understand how gates solve gradient problems\n",
        "4. **Apply both architectures to real stock data** and compare performance\n",
        "\n",
        "## Assignment Overview\n",
        "This assignment follows a progressive learning approach:\n",
        "1. Start with basic RNN implementation and theory\n",
        "2. Demonstrate vanishing/exploding gradient problems through experiments\n",
        "3. Implement basic LSTM network\n",
        "4. Apply both models to real-world stock prediction using PyTorch\n",
        "5. Compare and analyze results\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"yfinance version: {yf.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1-header"
      },
      "source": [
        "## Part 1: Basic RNN Implementation\n",
        "\n",
        "### 1.1 Understanding RNN Architecture\n",
        "\n",
        "Before implementing RNNs, let's understand their fundamental concept. Unlike feedforward neural networks, RNNs have a \"memory\" that allows them to process sequential data by maintaining hidden states.\n",
        "\n",
        "**Key RNN Equations:**\n",
        "- $h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$\n",
        "- $y_t = W_{hy} h_t + b_y$\n",
        "\n",
        "Where:\n",
        "- $h_t$: hidden state at time $t$\n",
        "- $x_t$: input at time $t$  \n",
        "- $y_t$: output at time $t$\n",
        "- $W_{hh}, W_{xh}, W_{hy}$: weight matrices\n",
        "- $b_h, b_y$: bias vectors\n",
        "\n",
        "### 1.2 Implement RNN from Scratch\n",
        "\n",
        "**Task**: Implement a simple RNN class that can process sequences and maintain hidden states.\n",
        "\n",
        "**Learning Goals**:\n",
        "- Understand how hidden states carry information across time steps\n",
        "- See how RNNs process sequential data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnn-implementation"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN:\n",
        "    \"\"\"A simple RNN implementation from scratch\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights with Xavier initialization\n",
        "        self.W_xh = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / (input_size + hidden_size))\n",
        "        self.W_hh = np.random.randn(hidden_size, hidden_size) * np.sqrt(2.0 / (hidden_size + hidden_size))\n",
        "        self.W_hy = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / (hidden_size + output_size))\n",
        "\n",
        "        # Initialize biases\n",
        "        self.b_h = np.zeros((hidden_size, 1))\n",
        "        self.b_y = np.zeros((output_size, 1))\n",
        "\n",
        "        # Initialize hidden state\n",
        "        self.h = np.zeros((hidden_size, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the RNN\"\"\"\n",
        "\n",
        "        \"TODO: YOUR CODE HERE\"\n",
        "        # Update hidden state\n",
        "\n",
        "        # Compute output\n",
        "\n",
        "        return y, self.h.copy()\n",
        "\n",
        "    def reset_hidden(self):\n",
        "        \"\"\"Reset hidden state to zeros\"\"\"\n",
        "        self.h = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"Get all parameters for visualization\"\"\"\n",
        "        return {\n",
        "            'W_xh': self.W_xh,\n",
        "            'W_hh': self.W_hh,\n",
        "            'W_hy': self.W_hy,\n",
        "            'b_h': self.b_h,\n",
        "            'b_y': self.b_y\n",
        "        }\n",
        "\n",
        "# Test the RNN implementation\n",
        "print(\"Creating RNN with input_size=1, hidden_size=3, output_size=1\")\n",
        "rnn = SimpleRNN(input_size=1, hidden_size=3, output_size=1)\n",
        "\n",
        "# Test with a simple sequence\n",
        "test_input = np.array([[0.5], [0.3], [0.8], [0.2]])\n",
        "print(f\"\\nTest input sequence: {test_input.flatten()}\")\n",
        "\n",
        "# Forward pass\n",
        "outputs = []\n",
        "hidden_states = []\n",
        "\n",
        "for i, x in enumerate(test_input):\n",
        "    x = x.reshape(-1, 1)  # Ensure column vector\n",
        "    y, h = rnn.forward(x)\n",
        "    outputs.append(y[0, 0])\n",
        "    hidden_states.append(h.flatten())\n",
        "    print(f\"Step {i+1}: Input={x[0,0]:.3f}, Output={y[0,0]:.3f}, Hidden={h.flatten()}\")\n",
        "\n",
        "print(f\"\\nFinal outputs: {outputs}\")\n",
        "print(\"RNN implementation successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2-header"
      },
      "source": [
        "## Part 2: Vanishing/Exploding Gradient Problem\n",
        "\n",
        "### 2.1 Understanding the Problem\n",
        "\n",
        "The vanishing gradient problem is one of the most significant challenges in training RNNs. Let's understand what causes this issue:\n",
        "\n",
        "**Mathematical Root Cause:**\n",
        "- In RNNs, gradients are computed using backpropagation through time (BPTT)\n",
        "- The gradient at time step $t$ depends on the product of gradients from all future time steps\n",
        "- If the gradient magnitude is consistently less than 1, it vanishes exponentially\n",
        "- If the gradient magnitude is consistently greater than 1, it explodes exponentially\n",
        "\n",
        "**Why This Happens:**\n",
        "- The `tanh` activation function has a maximum derivative of 1.0\n",
        "- Weight matrices are typically initialized with small values\n",
        "- The product of many small numbers becomes extremely small\n",
        "\n",
        "### 2.2 Empirical Demonstration\n",
        "\n",
        "**Task**: Visualize how gradients behave differently with different weight magnitudes and sequence lengths.\n",
        "\n",
        "**Learning Goals**:\n",
        "- Understand why gradients vanish/explode or explode in RNNs\n",
        "- See the exponential nature of gradient decay/growth\n",
        "- Recognize the impact of sequence length on gradient problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gradient-demonstration"
      },
      "outputs": [],
      "source": [
        "def demonstrate_gradient_problems():\n",
        "    \"\"\"Demonstrate how gradients vanish or explode in RNNs\"\"\"\n",
        "\n",
        "    def tanh_derivative(x):\n",
        "        \"\"\"Derivative of tanh function\"\"\"\n",
        "        return 1 - np.tanh(x)**2\n",
        "\n",
        "    # Simulate gradient flow through time\n",
        "    sequence_lengths = [5, 10, 20, 50]\n",
        "    weight_values = [0.5, 0.8, 0.9, 1.0, 1.1, 1.3]  # Different weight magnitudes\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Vanishing and Exploding Gradient Problem in RNNs', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Gradient magnitude over time for different sequence lengths\n",
        "    ax1 = axes[0, 0]\n",
        "    for length in sequence_lengths:\n",
        "        time_steps = range(length)\n",
        "        # Simulate gradient: starts at 1, multiplied by weight * tanh_derivative each step\n",
        "        gradient = [1.0]\n",
        "        for t in range(1, length):\n",
        "            # Assume tanh_derivative ≈ 0.5 on average, weight = 0.8\n",
        "            gradient.append(gradient[-1] * 0.8 * 0.5)\n",
        "\n",
        "        ax1.plot(time_steps, gradient, label=f'Length {length}', linewidth=2)\n",
        "\n",
        "    ax1.set_title('Gradient Magnitude Over Time (Vanishing)')\n",
        "    ax1.set_xlabel('Time Step')\n",
        "    ax1.set_ylabel('Gradient Magnitude')\n",
        "    ax1.set_yscale('log')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Effect of different weight magnitudes\n",
        "    ax2 = axes[0, 1]\n",
        "    sequence_length = 20\n",
        "    time_steps = range(sequence_length)\n",
        "\n",
        "    for weight in weight_values:\n",
        "        gradient = [1.0]\n",
        "        for t in range(1, sequence_length):\n",
        "            gradient.append(gradient[-1] * weight * 0.5)  # tanh_derivative ≈ 0.5\n",
        "\n",
        "        ax2.plot(time_steps, gradient, label=f'Weight {weight}', linewidth=2)\n",
        "\n",
        "    ax2.set_title('Effect of Weight Magnitude')\n",
        "    ax2.set_xlabel('Time Step')\n",
        "    ax2.set_ylabel('Gradient Magnitude')\n",
        "    ax2.set_yscale('log')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Tanh derivative visualization\n",
        "    ax3 = axes[1, 0]\n",
        "    x_range = np.linspace(-3, 3, 100)\n",
        "    tanh_values = np.tanh(x_range)\n",
        "    tanh_deriv_values = tanh_derivative(x_range)\n",
        "\n",
        "    ax3.plot(x_range, tanh_values, label='tanh(x)', linewidth=2)\n",
        "    ax3.plot(x_range, tanh_deriv_values, label='tanh\\'(x)', linewidth=2)\n",
        "    ax3.set_title('Tanh Function and Its Derivative')\n",
        "    ax3.set_xlabel('x')\n",
        "    ax3.set_ylabel('Value')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Gradient flow comparison\n",
        "    ax4 = axes[1, 1]\n",
        "    time_steps = range(30)\n",
        "\n",
        "    # Vanishing gradient (weight < 2)\n",
        "    vanishing_grad = [1.0]\n",
        "    for t in range(1, 30):\n",
        "        vanishing_grad.append(vanishing_grad[-1] * 0.8 * 0.5)\n",
        "\n",
        "    # Exploding gradient (weight > 2)\n",
        "    exploding_grad = [1.0]\n",
        "    for t in range(1, 30):\n",
        "        exploding_grad.append(exploding_grad[-1] * 2.5 * 0.5)\n",
        "\n",
        "    ax4.plot(time_steps, vanishing_grad, label='Vanishing (w=0.8)', linewidth=2)\n",
        "    ax4.plot(time_steps, exploding_grad, label='Exploding (w=2.5)', linewidth=2)\n",
        "    ax4.set_title('Vanishing vs Exploding Gradients')\n",
        "    ax4.set_xlabel('Time Step')\n",
        "    ax4.set_ylabel('Gradient Magnitude')\n",
        "    ax4.set_yscale('log')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the demonstration\n",
        "demonstrate_gradient_problems()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3-header"
      },
      "source": [
        "## Part 3: LSTM Implementation and Gates\n",
        "\n",
        "### 3.1 Understanding LSTM Architecture\n",
        "\n",
        "LSTMs solve the vanishing gradient problem through a sophisticated gating mechanism. Let's understand the key components:\n",
        "\n",
        "**LSTM Gates:**\n",
        "1. **Forget Gate** ($f_t$): Decides what information to discard from cell state\n",
        "2. **Input Gate** ($i_t$): Decides what new information to store in cell state  \n",
        "3. **Output Gate** ($o_t$): Decides what parts of cell state to output\n",
        "\n",
        "**Key Equations:**\n",
        "- $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
        "- $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
        "- $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
        "- $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n",
        "- $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
        "- $h_t = o_t * \\tanh(C_t)$\n",
        "\n",
        "### 3.2 Implement LSTM from Scratch\n",
        "\n",
        "**Task**: Implement a simple LSTM class that demonstrates the gating mechanism.\n",
        "\n",
        "**Learning Goals**:\n",
        "- Understand how gates control information flow\n",
        "- See how cell state maintains long-term memory\n",
        "- Visualize gate behavior during sequence processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lstm-implementation"
      },
      "outputs": [],
      "source": [
        "class SimpleLSTM:\n",
        "    \"\"\"A simple LSTM implementation from scratch\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights for all gates and cell state\n",
        "        # We'll combine input and hidden states: [h_{t-1}, x_t]\n",
        "        combined_size = hidden_size + input_size\n",
        "\n",
        "        # Forget gate weights\n",
        "        self.W_f = np.random.randn(hidden_size, combined_size) * np.sqrt(2.0 / combined_size)\n",
        "        self.b_f = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Input gate weights\n",
        "        self.W_i = np.random.randn(hidden_size, combined_size) * np.sqrt(2.0 / combined_size)\n",
        "        self.b_i = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Candidate values weights\n",
        "        self.W_C = np.random.randn(hidden_size, combined_size) * np.sqrt(2.0 / combined_size)\n",
        "        self.b_C = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Output gate weights\n",
        "        self.W_o = np.random.randn(hidden_size, combined_size) * np.sqrt(2.0 / combined_size)\n",
        "        self.b_o = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Output layer weights\n",
        "        self.W_y = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
        "        self.b_y = np.zeros((output_size, 1))\n",
        "\n",
        "        # Initialize hidden state and cell state\n",
        "        self.h = np.zeros((hidden_size, 1))\n",
        "        self.C = np.zeros((hidden_size, 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the LSTM\"\"\"\n",
        "\n",
        "        \"TODO: YOUR CODE HERE\"\n",
        "\n",
        "        return y, self.h.copy(), self.C.copy()\n",
        "\n",
        "    def reset_states(self):\n",
        "        \"\"\"Reset hidden state and cell state to zeros\"\"\"\n",
        "        self.h = np.zeros((self.hidden_size, 1))\n",
        "        self.C = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "    def get_gates(self, x):\n",
        "        \"\"\"Get gate values for visualization\"\"\"\n",
        "        combined = np.vstack([self.h, x])\n",
        "\n",
        "        f_t = self.sigmoid(self.W_f @ combined + self.b_f)\n",
        "        i_t = self.sigmoid(self.W_i @ combined + self.b_i)\n",
        "        C_tilde = np.tanh(self.W_C @ combined + self.b_C)\n",
        "        o_t = self.sigmoid(self.W_o @ combined + self.b_o)\n",
        "\n",
        "        return f_t, i_t, C_tilde, o_t\n",
        "\n",
        "# Test the LSTM implementation\n",
        "print(\"Creating LSTM with input_size=1, hidden_size=4, output_size=1\")\n",
        "lstm = SimpleLSTM(input_size=1, hidden_size=4, output_size=1)\n",
        "\n",
        "# Test with a simple sequence\n",
        "test_input = np.array([[0.5], [0.3], [0.8], [0.2]])\n",
        "print(f\"\\nTest input sequence: {test_input.flatten()}\")\n",
        "\n",
        "# Forward pass\n",
        "outputs = []\n",
        "hidden_states = []\n",
        "cell_states = []\n",
        "gate_values = []\n",
        "\n",
        "for i, x in enumerate(test_input):\n",
        "    x = x.reshape(-1, 1)  # Ensure column vector\n",
        "    y, h, C = lstm.forward(x)\n",
        "    f_t, i_t, C_tilde, o_t = lstm.get_gates(x)\n",
        "\n",
        "    outputs.append(y[0, 0])\n",
        "    hidden_states.append(h.flatten())\n",
        "    cell_states.append(C.flatten())\n",
        "    gate_values.append({\n",
        "        'forget': f_t.flatten(),\n",
        "        'input': i_t.flatten(),\n",
        "        'candidate': C_tilde.flatten(),\n",
        "        'output': o_t.flatten()\n",
        "    })\n",
        "\n",
        "    print(f\"Step {i+1}: Input={x[0,0]:.3f}, Output={y[0,0]:.3f}\")\n",
        "    print(f\"  Hidden: {h.flatten()}\")\n",
        "    print(f\"  Cell: {C.flatten()}\")\n",
        "    print(f\"  Gates - Forget: {f_t.flatten()}, Input: {i_t.flatten()}, Output: {o_t.flatten()}\")\n",
        "\n",
        "print(f\"\\nFinal outputs: {outputs}\")\n",
        "print(\"LSTM implementation successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4-header"
      },
      "source": [
        "## Part 4: PyTorch Implementation for Stock Data\n",
        "\n",
        "### 4.1 Data Preparation\n",
        "\n",
        "**Task**: Download and preprocess real stock data for training RNN and LSTM models.\n",
        "\n",
        "**Learning Goals**:\n",
        "- Learn how to handle real-world time series data\n",
        "- Understand data preprocessing for sequence models\n",
        "- Create proper train/validation/test splits for time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data-preparation"
      },
      "outputs": [],
      "source": [
        "def download_stock_data(symbol='AAPL', period='5y'):\n",
        "    \"\"\"\n",
        "    Download stock price data from Yahoo Finance\n",
        "\n",
        "    Args:\n",
        "        symbol (str): Stock symbol (e.g., 'AAPL', 'MSFT', 'TSLA')\n",
        "        period (str): Time period ('1y', '2y', '3y', etc.)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: Stock price data\n",
        "    \"\"\"\n",
        "    print(f\"Downloading {symbol} stock data for {period}...\")\n",
        "\n",
        "    # Download data\n",
        "    ticker = yf.Ticker(symbol)\n",
        "    data = ticker.history(period=period)\n",
        "\n",
        "    if data.empty:\n",
        "        print(f\"Error: No data found for symbol {symbol}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Successfully downloaded {len(data)} days of data\")\n",
        "    print(f\"Date range: {data.index[0].strftime('%Y-%m-%d')} to {data.index[-1].strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_stock_data(data, sequence_length=30):\n",
        "    \"\"\"\n",
        "    Preprocess stock data for RNN/LSTM training\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Raw stock data\n",
        "        sequence_length (int): Number of days to use for prediction\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X_train, X_val, X_test, y_train, y_val, y_test, scaler)\n",
        "    \"\"\"\n",
        "    # Use only closing prices\n",
        "    prices = data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "    # Normalize data to [0, 1] range\n",
        "    scaler = MinMaxScaler()\n",
        "    prices_normalized = scaler.fit_transform(prices)\n",
        "\n",
        "    # Create sequences\n",
        "    X, y = [], []\n",
        "    for i in range(len(prices_normalized) - sequence_length):\n",
        "        X.append(prices_normalized[i:i+sequence_length])\n",
        "        y.append(prices_normalized[i+sequence_length])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Split data: 70% train, 15% validation, 15% test\n",
        "    train_size = int(0.7 * len(X))\n",
        "    val_size = int(0.15 * len(X))\n",
        "\n",
        "    X_train = X[:train_size]\n",
        "    X_val = X[train_size:train_size + val_size]\n",
        "    X_test = X[train_size + val_size:]\n",
        "\n",
        "    y_train = y[:train_size]\n",
        "    y_val = y[train_size:train_size + val_size]\n",
        "    y_test = y[train_size + val_size:]\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
        "\n",
        "# Download and preprocess data\n",
        "stock_data = download_stock_data('AAPL', '5y')\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, scaler = preprocess_stock_data(stock_data)\n",
        "\n",
        "print(\"\\nData preprocessing complete!\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Validation samples: {X_val.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "print(f\"Sequence length: {X_train.shape[1]}\")\n",
        "print(f\"Features: {X_train.shape[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pytorch-models-header"
      },
      "source": [
        "### 4.2 PyTorch RNN and LSTM Models\n",
        "\n",
        "**Task**: Implement RNN and LSTM models using PyTorch for stock price prediction.\n",
        "\n",
        "**Learning Goals**:\n",
        "- Learn PyTorch implementation of RNNs and LSTMs\n",
        "- Understand model architecture design for time series\n",
        "- Implement proper training loops with validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pytorch-models"
      },
      "outputs": [],
      "source": [
        "class StockRNN(nn.Module):\n",
        "    \"\"\"PyTorch RNN model for stock price prediction\"\"\"\n",
        "\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):\n",
        "        super(StockRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        \"TODO: YOUR CODE HERE\"\n",
        "        # RNN layer using Pytorch\n",
        "        self.rnn =\n",
        "\n",
        "        # Output layer\n",
        "        self.fc =\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # RNN forward pass\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        # Take the last output\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class StockLSTM(nn.Module):\n",
        "    \"\"\"PyTorch LSTM model for stock price prediction\"\"\"\n",
        "\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):\n",
        "        super(StockLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        \"TODO: YOUR CODE HERE\"\n",
        "        # LSTM layer using Pytorch\n",
        "        self.lstm =\n",
        "\n",
        "        # Output layer\n",
        "        self.fc =\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Take the last output\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "def create_tensors(X_train, X_val, X_test, y_train, y_val, y_test):\n",
        "    \"\"\"Convert numpy arrays to PyTorch tensors\"\"\"\n",
        "    X_train_tensor = torch.FloatTensor(X_train)\n",
        "    X_val_tensor = torch.FloatTensor(X_val)\n",
        "    X_test_tensor = torch.FloatTensor(X_test)\n",
        "    y_train_tensor = torch.FloatTensor(y_train)\n",
        "    y_val_tensor = torch.FloatTensor(y_val)\n",
        "    y_test_tensor = torch.FloatTensor(y_test)\n",
        "\n",
        "    return X_train_tensor, X_val_tensor, X_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor\n",
        "\n",
        "\n",
        "# Create tensors\n",
        "X_train_tensor, X_val_tensor, X_test_tensor, y_train_tensor, y_val_tensor, y_test_tensor = create_tensors(\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"PyTorch models and data loaders created successfully!\")\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-header"
      },
      "source": [
        "### 4.3 Training and Evaluation\n",
        "\n",
        "**Task**: Train both RNN and LSTM models and compare their performance.\n",
        "\n",
        "**Learning Goals**:\n",
        "- Learn how to train sequence models effectively\n",
        "- Understand evaluation metrics for time series prediction\n",
        "- Compare RNN vs LSTM performance on real data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-function"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001, device='cpu'):\n",
        "    \"\"\"Train a PyTorch model\"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            \"TODO: YOUR CODE HERE\"\n",
        "            \"Set up Pytorch optimizer and perform gradient descent step\"\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, scaler, device='cpu'):\n",
        "    \"\"\"Evaluate a trained model\"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            actuals.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    actuals = np.array(actuals)\n",
        "\n",
        "    # Convert back to original scale\n",
        "    predictions_original = scaler.inverse_transform(predictions)\n",
        "    actuals_original = scaler.inverse_transform(actuals)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(actuals_original, predictions_original)\n",
        "    mae = mean_absolute_error(actuals_original, predictions_original)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    return predictions_original, actuals_original, mse, mae, rmse\n",
        "\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize models\n",
        "rnn_model = StockRNN(input_size=1, hidden_size=64, num_layers=2, output_size=1)\n",
        "lstm_model = StockLSTM(input_size=1, hidden_size=64, num_layers=2, output_size=1)\n",
        "\n",
        "print(\"\\nTraining RNN model...\")\n",
        "rnn_train_losses, rnn_val_losses = train_model(rnn_model, train_loader, val_loader, num_epochs=100, device=device)\n",
        "\n",
        "print(\"\\nTraining LSTM model...\")\n",
        "lstm_train_losses, lstm_val_losses = train_model(lstm_model, train_loader, val_loader, num_epochs=100, device=device)\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part5-header"
      },
      "source": [
        "## Part 5: Comparison and Analysis\n",
        "\n",
        "### 5.1 Performance Comparison\n",
        "\n",
        "**Task**: Evaluate and compare the performance of RNN and LSTM models.\n",
        "\n",
        "**Learning Goals**:\n",
        "- Understand how to measure model performance for time series\n",
        "- Analyze the differences between RNN and LSTM performance\n",
        "- Visualize predictions vs actual values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation"
      },
      "outputs": [],
      "source": [
        "# Evaluate both models\n",
        "print(\"Evaluating RNN model...\")\n",
        "rnn_predictions, rnn_actuals, rnn_mse, rnn_mae, rnn_rmse = evaluate_model(rnn_model, test_loader, scaler, device)\n",
        "\n",
        "print(\"Evaluating LSTM model...\")\n",
        "lstm_predictions, lstm_actuals, lstm_mse, lstm_mae, lstm_rmse = evaluate_model(lstm_model, test_loader, scaler, device)\n",
        "\n",
        "# Print performance metrics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "print(f\"RNN  - MSE: {rnn_mse:.4f}, MAE: {rnn_mae:.4f}, RMSE: {rnn_rmse:.4f}\")\n",
        "print(f\"LSTM - MSE: {lstm_mse:.4f}, MAE: {lstm_mae:.4f}, RMSE: {lstm_rmse:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Calculate improvement\n",
        "mse_improvement = ((rnn_mse - lstm_mse) / rnn_mse) * 100\n",
        "mae_improvement = ((rnn_mae - lstm_mae) / rnn_mae) * 100\n",
        "rmse_improvement = ((rnn_rmse - lstm_rmse) / rnn_rmse) * 100\n",
        "\n",
        "print(f\"\\nLSTM Improvement over RNN:\")\n",
        "print(f\"MSE:  {mse_improvement:.2f}%\")\n",
        "print(f\"MAE:  {mae_improvement:.2f}%\")\n",
        "print(f\"RMSE: {rmse_improvement:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('RNN vs LSTM Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Training curves\n",
        "ax1 = axes[0, 0]\n",
        "epochs = range(1, len(rnn_train_losses) + 1)\n",
        "ax1.plot(epochs, rnn_train_losses, 'b-', label='RNN Train', linewidth=2)\n",
        "ax1.plot(epochs, rnn_val_losses, 'b--', label='RNN Val', linewidth=2)\n",
        "ax1.plot(epochs, lstm_train_losses, 'r-', label='LSTM Train', linewidth=2)\n",
        "ax1.plot(epochs, lstm_val_losses, 'r--', label='LSTM Val', linewidth=2)\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Predictions vs Actual (RNN)\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(rnn_actuals, 'b-', label='Actual', linewidth=2)\n",
        "ax2.plot(rnn_predictions, 'r--', label='RNN Prediction', linewidth=2)\n",
        "ax2.set_title('RNN Predictions vs Actual')\n",
        "ax2.set_xlabel('Time Steps')\n",
        "ax2.set_ylabel('Stock Price ($)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Predictions vs Actual (LSTM)\n",
        "ax3 = axes[1, 0]\n",
        "ax3.plot(lstm_actuals, 'b-', label='Actual', linewidth=2)\n",
        "ax3.plot(lstm_predictions, 'g--', label='LSTM Prediction', linewidth=2)\n",
        "ax3.set_title('LSTM Predictions vs Actual')\n",
        "ax3.set_xlabel('Time Steps')\n",
        "ax3.set_ylabel('Stock Price ($)')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Performance metrics comparison\n",
        "ax4 = axes[1, 1]\n",
        "metrics = ['MSE', 'MAE', 'RMSE']\n",
        "rnn_values = [rnn_mse, rnn_mae, rnn_rmse]\n",
        "lstm_values = [lstm_mse, lstm_mae, lstm_rmse]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "ax4.bar(x - width/2, rnn_values, width, label='RNN', alpha=0.8)\n",
        "ax4.bar(x + width/2, lstm_values, width, label='LSTM', alpha=0.8)\n",
        "ax4.set_title('Performance Metrics Comparison')\n",
        "ax4.set_xlabel('Metrics')\n",
        "ax4.set_ylabel('Value')\n",
        "ax4.set_xticks(x)\n",
        "ax4.set_xticklabels(metrics)\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final analysis\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nKey Findings:\")\n",
        "print(f\"1. LSTM outperforms RNN by {mse_improvement:.1f}% in MSE\")\n",
        "print(f\"2. LSTM outperforms RNN by {mae_improvement:.1f}% in MAE\")\n",
        "print(f\"3. LSTM outperforms RNN by {rmse_improvement:.1f}% in RMSE\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO: YOUR CODE HERE\n",
        "\n",
        "#  Try different hyperparameters (hidden size, number of layers, learning rate). Describe what you observe when varying these hyperparameters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ihsjaf4lgulN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}