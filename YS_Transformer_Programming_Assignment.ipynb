{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Y326s/ECE6397HW/blob/main/Transformer_Programming_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-gzZLC8UMC2"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "unexpected indent (297087878.py, line 53)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mS\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "TRANSFORMER ARCHITECTURE ASSIGNMENT\n",
        "=====================================\n",
        "Learning Objective: Understand the key components of transformer models\n",
        "\n",
        "In this assignment, you'll implement simplified versions of the core\n",
        "transformer components: Self-Attention, Multi-Head Attention, and a\n",
        "basic Transformer Block.\n",
        "\n",
        "PART 1: Self-Attention Mechanism\n",
        "---------------------------------------------\n",
        "The self-attention mechanism allows each position in a sequence to attend\n",
        "to all positions. It computes attention scores using Query, Key, and Value matrices.\n",
        "\n",
        "Formula: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
        "\n",
        "Complete the self_attention() function below.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Helper function: Compute softmax along the last dimension.\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "def self_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Implement scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        Q: Query matrix of shape (seq_len, d_k)\n",
        "        K: Key matrix of shape (seq_len, d_k)\n",
        "        V: Value matrix of shape (seq_len, d_v)  # Office hour: d_v can be different from d_k, see original paper for details\n",
        "\n",
        "    Returns:\n",
        "        output: Attention output of shape (seq_len, d_v)\n",
        "        attention_weights: Attention weights of shape (seq_len, seq_len)\n",
        "\n",
        "    TODO: Implement the following steps:\n",
        "    1. Compute attention scores by matrix multiplication of Q and K^T\n",
        "    2. Scale the scores by dividing by sqrt(d_k)\n",
        "    3. Apply softmax to get attention weights\n",
        "    4. Multiply attention weights with V to get output\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Step 1: Compute QK^T\n",
        "    # scores = np.einsum(\"iq,ik->iqk\",Q,K)  # Replace with your implementation\n",
        "    scores = np.matmul(Q,K.T)  # Replace with your implementation\n",
        "        # Office hour: Here we use matrix form to deal with all the sequence steps at one time.\n",
        "        # So each of k0~k2 and q0~q2 in the lecture video is actually a feature vector of length d_k\n",
        "        # eij refers to the attention score for each sequence location conbination\n",
        "\n",
        "    # Step 2: Scale by sqrt(d_k)\n",
        "    scaled_scores = scores/np.sqrt(d_k)  # Replace with your implementation\n",
        "\n",
        "    # Step 3: Apply softmax\n",
        "    attention_weights = softmax(scaled_scores)  # Replace with your implementation\n",
        "\n",
        "    # Step 4: Multiply with V\n",
        "    output = np.matmul(attention_weights,V)  # Replace with your implementation\n",
        "\n",
        "    return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SPn7iin6Uftj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "PART 2: Multi-Head Attention\n",
        "-----------------------------------------\n",
        "Multi-head attention runs multiple attention mechanisms in parallel,\n",
        "allowing the model to focus on different aspects of the input.\n",
        "\n",
        "Complete the multi_head_attention() function below.\n",
        "\"\"\"\n",
        "\n",
        "def multi_head_attention(X, num_heads, d_model):\n",
        "    \"\"\"\n",
        "    Implement multi-head attention (simplified version).\n",
        "\n",
        "    Args:\n",
        "        X: Input matrix of shape (seq_len, d_model)\n",
        "        num_heads: Number of attention heads\n",
        "        d_model: Model dimension (must be divisible by num_heads)\n",
        "\n",
        "    Returns:\n",
        "        output: Multi-head attention output of shape (seq_len, d_model)\n",
        "\n",
        "    TODO: Implement the following steps:\n",
        "    1. Check that d_model is divisible by num_heads\n",
        "    2. Calculate d_k (dimension per head)\n",
        "    3. Create random projection matrices for Q, K, V for each head\n",
        "    4. For each head:\n",
        "       - Project X to get Q, K, V\n",
        "       - Apply self_attention\n",
        "       - Store the result\n",
        "    5. Concatenate all head outputs\n",
        "    6. Apply final linear projection (use random matrix)\n",
        "    \"\"\"\n",
        "    seq_len = X.shape[0]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Step 1 & 2: Verify divisibility and compute d_k\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "    d_k = int(d_model/num_heads)  # Replace with your implementation\n",
        "\n",
        "    # Step 3: Initialize projection matrices (we'll use random matrices for simplicity)\n",
        "    np.random.seed(42)\n",
        "    W_q = [np.random.randn(d_model, d_k) for _ in range(num_heads)]\n",
        "    W_k = [np.random.randn(d_model, d_k) for _ in range(num_heads)]\n",
        "    W_v = [np.random.randn(d_model, d_k) for _ in range(num_heads)]\n",
        "\n",
        "    # Step 4: Process each head\n",
        "    head_outputs = []\n",
        "    for i in range(num_heads):\n",
        "        # Project X to get Q, K, V for this head\n",
        "        Q = W_q[i]  # Replace with your implementation\n",
        "        K = W_k[i]  # Replace with your implementation\n",
        "        V = W_v[i]  # Replace with your implementation\n",
        "\n",
        "        # Apply attention\n",
        "        head_output, _ = self_attention(Q, K, V)\n",
        "        head_outputs.append(head_output)\n",
        "\n",
        "    # Step 5: Concatenate heads\n",
        "    multi_head_output = np.concatenate(head_outputs, axis=-1)  # Replace: concatenate head_outputs along last dimension\n",
        "\n",
        "    # Step 6: Final linear projection\n",
        "    W_o = np.random.randn(d_model, d_model)\n",
        "    output = np.matmul(multi_head_output,W_o)  # Replace with your implementation\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1XBukmeUiP3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "PART 3: Testing Your Implementation\n",
        "------------------------------------------------\n",
        "Write test code to verify your implementations work correctly.\n",
        "\n",
        "TODO:\n",
        "1. Create sample input data (e.g., seq_len=4, d_model=8)\n",
        "2. Test self_attention with Q, K, V matrices\n",
        "3. Verify attention weights sum to 1\n",
        "4. Test multi_head_attention with 2 heads\n",
        "5. Print shapes and sample outputs\n",
        "\"\"\"\n",
        "\n",
        "def test_transformer_components():\n",
        "    \"\"\"\n",
        "    Test your self_attention and multi_head_attention implementations.\n",
        "\n",
        "    TODO: Implement tests that:\n",
        "    - Create sample input data\n",
        "    - Call self_attention and verify output shape\n",
        "    - Verify attention weights sum to 1 for each query position\n",
        "    - Call multi_head_attention and verify output shape\n",
        "    - Print results\n",
        "    \"\"\"\n",
        "    print(\"Testing Transformer Components\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Test 1: Self-Attention\n",
        "    print(\"\\nTest 1: Self-Attention\")\n",
        "    seq_len = 4\n",
        "    d_k = 8\n",
        "    d_input = 128  # Office hour: d_input can be different from d_k and usually much large\n",
        "\n",
        "    x = np.random.uniform(0,1,(seq_len, d_input))\n",
        "    print(\"Input data x: {}\".format(x))\n",
        "\n",
        "    # Create sample Q, K, V matrices\n",
        "    np.random.seed(42)\n",
        "    Q = np.matmul(x,np.random.uniform(0,1,(d_input, d_k)))  # Create a (seq_len, d_k) matrix\n",
        "    K = np.matmul(x,np.random.uniform(0,1,(d_input, d_k)))  # Create a (seq_len, d_k) matrix\n",
        "    V = np.matmul(x,np.random.uniform(0,1,(d_input, d_k)))  # Create a (seq_len, d_k) matrix\n",
        "    # print(\"Q, K, and V:\\n{}\\n{}\\n{}\".format(Q,K,V))\n",
        "\n",
        "\n",
        "    # Call self_attention\n",
        "    output, attention_weights = self_attention(Q, K, V)\n",
        "\n",
        "    # Print shapes and verify\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "    print(f\"Attention weights sum (should be ~1.0): {attention_weights.sum(axis=1)}\")\n",
        "\n",
        "    # Test 2: Multi-Head Attention\n",
        "    print(\"\\nTest 2: Multi-Head Attention\")\n",
        "    # YOUR CODE HERE\n",
        "    mh_output = multi_head_attention(x, 2, d_k)\n",
        "    print(mh_output)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N_THqPbce7Me"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------------\n",
        "Implement a simple positional encoding function that adds position\n",
        "information to input embeddings.\n",
        "\n",
        "Formula:\n",
        "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "\"\"\"\n",
        "\n",
        "def positional_encoding(seq_len, d_model):\n",
        "    \"\"\"\n",
        "    Generate positional encoding matrix.\n",
        "\n",
        "    Args:\n",
        "        seq_len: Sequence length\n",
        "        d_model: Model dimension\n",
        "\n",
        "    Returns:\n",
        "        PE: Positional encoding matrix of shape (seq_len, d_model)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pe = np.zeros((seq_len,d_model))\n",
        "\n",
        "    if d_model%2 == 0:\n",
        "        for tt in range(seq_len):\n",
        "            for ii in range(d_model//2):\n",
        "                pe[tt, 2*ii] = np.sin(tt / 10000^(2*ii/d_model))\n",
        "                pe[tt, 2*ii+1] = np.cos(tt / 10000^(2*ii/d_model))\n",
        "    else:\n",
        "        for tt in range(seq_len):\n",
        "            for ii in range(d_model//2):\n",
        "                pe[tt, 2*ii] = np.sin(tt / 10000^(2*ii/d_model))\n",
        "                pe[tt, 2*ii+1] = np.cos(tt / 10000^(2*ii/d_model))  \n",
        "            pe[tt, -1] = np.sin(tt / 10000^(2*(d_model//2+1))/d_model)\n",
        "\n",
        "    return pe\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8zRXzeuUrQw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Transformer Components\n",
            "==================================================\n",
            "\n",
            "Test 1: Self-Attention\n",
            "Input data x: [[5.36479015 4.37342159 5.89697959 1.70830575 7.79399611 6.50259255\n",
            "  3.93249942 6.9587027 ]\n",
            " [0.80988727 6.03765303 6.20410189 3.7848692  3.832064   6.04052907\n",
            "  1.25483865 8.11173365]\n",
            " [4.98387541 5.99033632 0.78399175 8.85804163 1.85198544 9.91954737\n",
            "  3.67840759 6.75268621]\n",
            " [7.26834269 9.33131314 9.17284295 8.30206495 2.10423124 5.03913595\n",
            "  7.20707548 1.7403051 ]]\n",
            "Output shape: (4, 8)\n",
            "Attention weights shape: (4, 4)\n",
            "Attention weights sum (should be ~1.0): [1. 1. 1. 1.]\n",
            "\n",
            "Test 2: Multi-Head Attention\n",
            "[[-3.30087530e-01 -2.14641338e+00  1.31508266e-02 -1.13012680e-01\n",
            "  -1.33430994e-01 -1.08964636e+00 -3.52519211e-02 -4.72074204e-01]\n",
            " [ 8.31800441e-01 -1.87662159e+00 -3.86084000e-01  4.93829077e-01\n",
            "  -1.34310512e-01 -1.19210570e-01 -9.04348552e-01 -5.80675375e-01]\n",
            " [ 1.25239291e+00 -1.94593590e+00 -5.06690960e-01  6.38845411e-01\n",
            "  -2.20831770e-01  3.56618245e-01 -6.80713714e-01 -3.00919871e-01]\n",
            " [ 1.15349977e+00 -2.40193218e+00 -8.56576468e-01  4.03975885e-01\n",
            "   1.13970073e-01 -2.31078699e-02 -1.00014766e+00 -1.28729342e+00]\n",
            " [ 3.54506693e-01 -3.35261270e+00  1.18198078e-01  1.17779878e-01\n",
            "  -1.73760478e+00 -5.29512350e-01 -2.58909999e-02 -9.68579156e-01]\n",
            " [ 9.25030543e-01 -4.46260515e+00 -7.58201233e-01 -1.99960160e-01\n",
            "  -7.50029729e-01 -9.29070236e-01  2.73045139e-01 -1.12385502e+00]\n",
            " [ 1.40070359e+00  8.95029781e-01 -7.78586254e-01 -2.06403887e-03\n",
            "   5.95157252e-01  1.55646439e+00 -1.11349556e+00  9.57862739e-01]\n",
            " [ 1.28847042e+00  1.21011728e-01 -8.21208819e-01  8.78203462e-01\n",
            "   1.38366809e+00  6.60157693e-01 -1.59755392e+00 -2.02071630e-02]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "SUBMISSION INSTRUCTIONS\n",
        "-----------------------\n",
        "1. Complete all TODO sections\n",
        "2. Test your code with the test function\n",
        "3. Answer these questions in comments:\n",
        "   a) Why do we scale attention scores by sqrt(d_k)?\n",
        "   b) What is the advantage of multi-head attention over single-head?\n",
        "   c) Why do transformers need positional encoding?\n",
        "\n",
        "4. Include example output from your tests\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run your tests here\n",
        "    test_transformer_components()\n",
        "\n",
        "    # Answer the questions:\n",
        "    \"\"\"\n",
        "    Q1: Why do we scale attention scores by sqrt(d_k)?\n",
        "    YOUR ANSWER:\n",
        "    The attention scores should be devided by sqrt(d_k) to keep \n",
        "    a small value after dot product between keys and queries.\n",
        "    So that the score for different features after applying softmax\n",
        "    will not have large relative difference in value.\n",
        "   \n",
        "\n",
        "    Q2: What is the advantage of multi-head attention over single-head?\n",
        "    YOUR ANSWER:\n",
        "    The multihead attention can capture different groups of features with different\n",
        "    key, querie, and value pairs. The multihead structure can bring stronger learning\n",
        "    ability for different kinds of features, which is beneficial like a multi-nueron layer\n",
        "    in MLP.\n",
        "\n",
        "    \n",
        "\n",
        "    Q3: Why do transformers need positional encoding?\n",
        "    YOUR ANSWER:\n",
        "    Because the attention block is positional invarient. However, the relative position for \n",
        "    different features are important in real-world tasks (e.g., different features in an image\n",
        "    are positional sensitive). So we need to encode the position into each features.\n",
        "\n",
        "    \"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
