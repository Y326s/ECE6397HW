{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(\"Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4349a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple example to understand convolution\n",
    "def visualize_convolution():\n",
    "    # Create a simple 5x5 image with a vertical line\n",
    "    image = torch.zeros(1, 1, 5, 5)\n",
    "    image[0, 0, :, 2] = 1  # Vertical line in the middle\n",
    "\n",
    "    # Create different filters\n",
    "    vertical_filter = torch.tensor([[[[-1, 0, 1],\n",
    "                                      [-1, 0, 1],\n",
    "                                      [-1, 0, 1]]]], dtype=torch.float32)\n",
    "\n",
    "    horizontal_filter = torch.tensor([[[[-1, -1, -1],\n",
    "                                        [0, 0, 0],\n",
    "                                        [1, 1, 1]]]], dtype=torch.float32)\n",
    "\n",
    "    # Apply convolution\n",
    "    conv_vertical = F.conv2d(image, vertical_filter, padding=1)\n",
    "    conv_horizontal = F.conv2d(image, horizontal_filter, padding=1)\n",
    " \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    axes[0].imshow(image[0, 0].numpy(), cmap='gray')\n",
    "    axes[0].set_title('Original Image\\\\n(Vertical Line)')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(conv_vertical[0, 0].numpy(), cmap='RdBu')\n",
    "    axes[1].set_title('Vertical Filter Response\\\\n(Detects vertical edges)')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(conv_horizontal[0, 0].numpy(), cmap='RdBu')\n",
    "    axes[2].set_title('Horizontal Filter Response\\\\n(Detects horizontal edges)')\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Key Observation: Different filters detect different patterns!\")\n",
    "    print(\"   - Vertical filter: Strong response to vertical lines\")\n",
    "    print(\"   - Horizontal filter: Strong response to horizontal lines\")\n",
    "\n",
    "visualize_convolution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6624098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Download and load datasets\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Visualize some samples\n",
    "def show_samples(loader, num_samples=8):\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "    for i in range(num_samples):\n",
    "        row, col = i // 4, i % 4\n",
    "        axes[row, col].imshow(images[i][0].numpy(), cmap='gray')\n",
    "        axes[row, col].set_title(f'Label: {labels[i].item()}')\n",
    "        axes[row, col].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\\\nSample MNIST images:\")\n",
    "show_samples(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0de744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple CNN for MNIST classification.\n",
    "    Architecture: Conv -> ReLU -> MaxPool -> Conv -> ReLU -> MaxPool -> FC\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 28x28 -> 14x14\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 14x14 -> 7x7\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # 64 channels * 7 * 7 = 3136\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = F.relu(self.conv1(x))  # 1x28x28 -> 32x28x28\n",
    "        x = self.pool1(x)          # 32x28x28 -> 32x14x14\n",
    "\n",
    "        # Second conv block\n",
    "        x = F.relu(self.conv2(x))  # 32x14x14 -> 64x14x14\n",
    "        x = self.pool2(x)          # 64x14x14 -> 64x7x7\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # 64x7x7 -> 3136\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))    # 3136 -> 128\n",
    "        x = self.dropout(x)        # Apply dropout\n",
    "        x = self.fc2(x)            # 128 -> 10\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create and test our CNN\n",
    "model = SimpleCNN()\n",
    "model.to(device)\n",
    "\n",
    "# Test with a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "images, labels = sample_batch  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(images)\n",
    "    predictions = torch.argmax(output, dim=1)  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "print(f\"Input shape: {images.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Sample predictions: {predictions[:5].cpu().numpy()}\")\n",
    "print(f\"Actual labels: {labels[:5].numpy()}\")\n",
    "print(\"SimpleCNN working correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ac7c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 9 (1914363508.py, line 10)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmodel.train():\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'for' statement on line 9\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=5, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train the CNN model\n",
    "    \"\"\"\n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Track training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            running_loss += loss.item()  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            _, predicted = torch.max(outputs.data, 1)  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            total += labels.size(0)  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            correct += (predicted == labels).sum().item()  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "        # Test phase\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
    "\n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_losses, train_accs, test_accs = train_model(model, train_loader, test_loader, epochs=5)\n",
    "print(\"\\\\n Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffa605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(train_losses, 'b-', label='Training Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(train_accs, 'b-', label='Training Accuracy')\n",
    "ax2.plot(test_accs, 'r-', label='Test Accuracy')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"Final test accuracy: {test_accs[-1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(model, layer_name='conv1'):\n",
    "    \"\"\"\n",
    "    Visualize the learned filters in the first convolutional layer\n",
    "    \"\"\"\n",
    "    # Get the first convolutional layer\n",
    "    conv_layer = getattr(model, layer_name)\n",
    "    filters = conv_layer.weight.data.cpu().numpy()\n",
    "\n",
    "    # Plot first 16 filters\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "    for i in range(16):\n",
    "        row, col = i // 4, i % 4\n",
    "        filter_img = filters[i, 0]  # First channel (grayscale)\n",
    "\n",
    "        # Normalize for visualization\n",
    "        filter_img = (filter_img - filter_img.min()) / (filter_img.max() - filter_img.min() + 1e-8)\n",
    "\n",
    "        axes[row, col].imshow(filter_img, cmap='gray')\n",
    "        axes[row, col].set_title(f'Filter {i+1}')\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.suptitle(f'Learned Filters in {layer_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_feature_maps(model, sample_image, layer_name='conv1'):\n",
    "    \"\"\"\n",
    "    Visualize feature maps for a sample image\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Hook to capture intermediate outputs\n",
    "    activations = {}\n",
    "    def hook_fn(module, input, output):\n",
    "        activations[layer_name] = output.detach()\n",
    "\n",
    "    # Register hook\n",
    "    conv_layer = getattr(model, layer_name)\n",
    "    hook = conv_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(sample_image.unsqueeze(0).to(device))\n",
    "\n",
    "    # Remove hook\n",
    "    hook.remove()\n",
    "\n",
    "    # Visualize feature maps\n",
    "    feature_maps = activations[layer_name].cpu().numpy()[0]  # First sample\n",
    "\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "    for i in range(32):  # Show first 32 feature maps\n",
    "        row, col = i // 8, i % 8\n",
    "        if i < feature_maps.shape[0]:\n",
    "            axes[row, col].imshow(feature_maps[i], cmap='viridis')\n",
    "            axes[row, col].set_title(f'FM {i+1}')\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.suptitle(f'Feature Maps from {layer_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize learned filters\n",
    "print(\"Visualizing learned filters...\")\n",
    "visualize_filters(model, 'conv1')\n",
    "\n",
    "# Get a sample image\n",
    "sample_images, sample_labels = next(iter(test_loader))\n",
    "sample_image = sample_images[0]\n",
    "\n",
    "# Show original image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sample_image[0].numpy(), cmap='gray')\n",
    "plt.title(f'Sample Image (Label: {sample_labels[0].item()})')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature maps\n",
    "print(\"Visualizing feature maps...\")\n",
    "visualize_feature_maps(model, sample_image, 'conv1')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
