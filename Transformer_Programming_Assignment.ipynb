{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Y326s/ECE6397HW/blob/main/Transformer_Programming_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-gzZLC8UMC2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TRANSFORMER ARCHITECTURE ASSIGNMENT\n",
        "=====================================\n",
        "Learning Objective: Understand the key components of transformer models\n",
        "\n",
        "In this assignment, you'll implement simplified versions of the core\n",
        "transformer components: Self-Attention, Multi-Head Attention, and a\n",
        "basic Transformer Block.\n",
        "\n",
        "PART 1: Self-Attention Mechanism\n",
        "---------------------------------------------\n",
        "The self-attention mechanism allows each position in a sequence to attend\n",
        "to all positions. It computes attention scores using Query, Key, and Value matrices.\n",
        "\n",
        "Formula: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
        "\n",
        "Complete the self_attention() function below.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Helper function: Compute softmax along the last dimension.\"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "def self_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Implement scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        Q: Query matrix of shape (seq_len, d_k)\n",
        "        K: Key matrix of shape (seq_len, d_k)\n",
        "        V: Value matrix of shape (seq_len, d_v)\n",
        "\n",
        "    Returns:\n",
        "        output: Attention output of shape (seq_len, d_v)\n",
        "        attention_weights: Attention weights of shape (seq_len, seq_len)\n",
        "\n",
        "    TODO: Implement the following steps:\n",
        "    1. Compute attention scores by matrix multiplication of Q and K^T\n",
        "    2. Scale the scores by dividing by sqrt(d_k)\n",
        "    3. Apply softmax to get attention weights\n",
        "    4. Multiply attention weights with V to get output\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Step 1: Compute QK^T\n",
        "    scores = None  # Replace with your implementation\n",
        "\n",
        "    # Step 2: Scale by sqrt(d_k)\n",
        "    scaled_scores = None  # Replace with your implementation\n",
        "\n",
        "    # Step 3: Apply softmax\n",
        "    attention_weights = None  # Replace with your implementation\n",
        "\n",
        "    # Step 4: Multiply with V\n",
        "    output = None  # Replace with your implementation\n",
        "\n",
        "    return output, attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "PART 2: Multi-Head Attention\n",
        "-----------------------------------------\n",
        "Multi-head attention runs multiple attention mechanisms in parallel,\n",
        "allowing the model to focus on different aspects of the input.\n",
        "\n",
        "Complete the multi_head_attention() function below.\n",
        "\"\"\"\n",
        "\n",
        "def multi_head_attention(X, num_heads, d_model):\n",
        "    \"\"\"\n",
        "    Implement multi-head attention (simplified version).\n",
        "\n",
        "    Args:\n",
        "        X: Input matrix of shape (seq_len, d_model)\n",
        "        num_heads: Number of attention heads\n",
        "        d_model: Model dimension (must be divisible by num_heads)\n",
        "\n",
        "    Returns:\n",
        "        output: Multi-head attention output of shape (seq_len, d_model)\n",
        "\n",
        "    TODO: Implement the following steps:\n",
        "    1. Check that d_model is divisible by num_heads\n",
        "    2. Calculate d_k (dimension per head)\n",
        "    3. Create random projection matrices for Q, K, V for each head\n",
        "    4. For each head:\n",
        "       - Project X to get Q, K, V\n",
        "       - Apply self_attention\n",
        "       - Store the result\n",
        "    5. Concatenate all head outputs\n",
        "    6. Apply final linear projection (use random matrix)\n",
        "    \"\"\"\n",
        "    seq_len = X.shape[0]\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Step 1 & 2: Verify divisibility and compute d_k\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "    d_k = None  # Replace with your implementation\n",
        "\n",
        "    # Step 3: Initialize projection matrices (we'll use random matrices for simplicity)\n",
        "    np.random.seed(42)\n",
        "    W_q = [np.random.randn(d_model, d_k) for _ in range(num_heads)]\n",
        "    W_k = [np.random.randn(d_model, d_k) for _ in range(num_heads)]\n",
        "    W_v = [np.random.randn(d_model, d_k) for _ in range(num_heads)]\n",
        "\n",
        "    # Step 4: Process each head\n",
        "    head_outputs = []\n",
        "    for i in range(num_heads):\n",
        "        # Project X to get Q, K, V for this head\n",
        "        Q = None  # Replace with your implementation\n",
        "        K = None  # Replace with your implementation\n",
        "        V = None  # Replace with your implementation\n",
        "\n",
        "        # Apply attention\n",
        "        head_output, _ = self_attention(Q, K, V)\n",
        "        head_outputs.append(head_output)\n",
        "\n",
        "    # Step 5: Concatenate heads\n",
        "    multi_head_output = None  # Replace: concatenate head_outputs along last dimension\n",
        "\n",
        "    # Step 6: Final linear projection\n",
        "    W_o = np.random.randn(d_model, d_model)\n",
        "    output = None  # Replace with your implementation\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "SPn7iin6Uftj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "PART 3: Testing Your Implementation\n",
        "------------------------------------------------\n",
        "Write test code to verify your implementations work correctly.\n",
        "\n",
        "TODO:\n",
        "1. Create sample input data (e.g., seq_len=4, d_model=8)\n",
        "2. Test self_attention with Q, K, V matrices\n",
        "3. Verify attention weights sum to 1\n",
        "4. Test multi_head_attention with 2 heads\n",
        "5. Print shapes and sample outputs\n",
        "\"\"\"\n",
        "\n",
        "def test_transformer_components():\n",
        "    \"\"\"\n",
        "    Test your self_attention and multi_head_attention implementations.\n",
        "\n",
        "    TODO: Implement tests that:\n",
        "    - Create sample input data\n",
        "    - Call self_attention and verify output shape\n",
        "    - Verify attention weights sum to 1 for each query position\n",
        "    - Call multi_head_attention and verify output shape\n",
        "    - Print results\n",
        "    \"\"\"\n",
        "    print(\"Testing Transformer Components\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # Test 1: Self-Attention\n",
        "    print(\"\\nTest 1: Self-Attention\")\n",
        "    seq_len = 4\n",
        "    d_k = 8\n",
        "\n",
        "    # Create sample Q, K, V matrices\n",
        "    np.random.seed(42)\n",
        "    Q = None  # Create a (seq_len, d_k) matrix\n",
        "    K = None  # Create a (seq_len, d_k) matrix\n",
        "    V = None  # Create a (seq_len, d_k) matrix\n",
        "\n",
        "    # Call self_attention\n",
        "    # output, attention_weights = self_attention(Q, K, V)\n",
        "\n",
        "    # Print shapes and verify\n",
        "    # print(f\"Output shape: {output.shape}\")\n",
        "    # print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "    # print(f\"Attention weights sum (should be ~1.0): {attention_weights.sum(axis=1)}\")\n",
        "\n",
        "    # Test 2: Multi-Head Attention\n",
        "    print(\"\\nTest 2: Multi-Head Attention\")\n",
        "    # YOUR CODE HERE\n",
        "\n"
      ],
      "metadata": {
        "id": "H1XBukmeUiP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------------\n",
        "Implement a simple positional encoding function that adds position\n",
        "information to input embeddings.\n",
        "\n",
        "Formula:\n",
        "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "\"\"\"\n",
        "\n",
        "def positional_encoding(seq_len, d_model):\n",
        "    \"\"\"\n",
        "    Generate positional encoding matrix.\n",
        "\n",
        "    Args:\n",
        "        seq_len: Sequence length\n",
        "        d_model: Model dimension\n",
        "\n",
        "    Returns:\n",
        "        PE: Positional encoding matrix of shape (seq_len, d_model)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "N_THqPbce7Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "SUBMISSION INSTRUCTIONS\n",
        "-----------------------\n",
        "1. Complete all TODO sections\n",
        "2. Test your code with the test function\n",
        "3. Answer these questions in comments:\n",
        "   a) Why do we scale attention scores by sqrt(d_k)?\n",
        "   b) What is the advantage of multi-head attention over single-head?\n",
        "   c) Why do transformers need positional encoding?\n",
        "\n",
        "4. Include example output from your tests\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run your tests here\n",
        "    test_transformer_components()\n",
        "\n",
        "    # Answer the questions:\n",
        "    \"\"\"\n",
        "    Q1: Why do we scale attention scores by sqrt(d_k)?\n",
        "    YOUR ANSWER:\n",
        "\n",
        "    Q2: What is the advantage of multi-head attention over single-head?\n",
        "    YOUR ANSWER:\n",
        "\n",
        "    Q3: Why do transformers need positional encoding?\n",
        "    YOUR ANSWER:\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "A8zRXzeuUrQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}